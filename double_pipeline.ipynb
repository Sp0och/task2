{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task: Classification into 4 classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea is to use CWT images of the data and perform classification using CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes:\n",
    "- Can't drop nans collectively because of different signal lengths\n",
    "- For the imbalanced datasets we can introduce some bias towards the under-represented values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-28 15:45:32.227099: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-28 15:45:32.457352: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-28 15:45:32.457414: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-11-28 15:45:39.544359: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-28 15:45:39.545027: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-28 15:45:39.545070: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from biosppy.signals import ecg\n",
    "from scipy import signal\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import â€“ takes a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# import X_train.csv dataset into a pandas dataframe\n",
    "X_train = pd.read_csv(\"X_train.csv\", index_col=\"id\")\n",
    "y_train = pd.read_csv(\"y_train.csv\", index_col=\"id\")\n",
    "X_test = pd.read_csv(\"X_test.csv\", index_col=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.164318\n",
      "16322\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(X_train.loc[0].dropna().to_numpy(dtype=\"float32\")))\n",
    "print(X_train.loc[0].dropna().to_numpy(dtype=\"float32\").shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get class representatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3030\n",
      "443\n",
      "1474\n",
      "170\n"
     ]
    }
   ],
   "source": [
    "y_train_np = y_train.to_numpy()\n",
    "ones = np.where(y_train_np == 0)[0]\n",
    "twos = np.where(y_train_np == 1)[0]\n",
    "threes = np.where(y_train_np == 2)[0]\n",
    "fours = np.where(y_train_np == 3)[0]\n",
    "print(len(ones))\n",
    "print(len(twos))\n",
    "print(len(threes))\n",
    "print(len(fours))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Wavelet transform:\n",
    "### Create CWT images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# widths = np.arange(1,31)\n",
    "# print(\"cwtmatrix shape (30 from width x # signal length\")\n",
    "# Class = 3\n",
    "# for i in range(len(threes)):\n",
    "#         print(i)\n",
    "#         # print(len(X_train.loc[threes[i]].dropna().to_numpy(dtype=\"float32\")))\n",
    "#         cwtmatr = signal.cwt(X_train.loc[threes[i]].dropna().to_numpy(dtype=\"float32\"), signal.ricker, widths)\n",
    "#         # print(cwtmatr.shape)\n",
    "#         plt.figure()\n",
    "#         plt.imshow(cwtmatr, extent=[-1, 1, 31, 1], cmap='PRGn', aspect='auto',\n",
    "#                 vmax=abs(cwtmatr).max(), vmin=-abs(cwtmatr).max())\n",
    "#         plt.axis('off')\n",
    "#         # plt.show()\n",
    "#         plt.savefig(\"3/cwtmatr_class{}_index{}.png\".format(Class, i), bbox_inches='tight', pad_inches=0)\n",
    "#         plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH = 334\n",
    "IMG_HEIGHT = 217\n",
    "def load_image(img, training):\n",
    "    '''Helper Function to load the image'''\n",
    "    img = tf.image.decode_png(tf.io.read_file(img), channels=3)\n",
    "    img = tf.cast(img, tf.float32)\n",
    "    img = img / 127.5 - 1\n",
    "    img = tf.image.resize(img, (IMG_HEIGHT, IMG_WIDTH))\n",
    "    if training:\n",
    "        img = tf.image.random_flip_left_right(img)\n",
    "        img = tf.image.random_flip_up_down(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 16:42:48.104928: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "image_1 = load_image(\"3/cwtmatr_class3_index0.png\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToDo: \n",
    "- Extract features from heartbeat templates and concatenate with scalar output from model\n",
    "- consider imbalanced dataset => add bias towards under-represented classes\n",
    "- train model\n",
    "- predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction: \n",
    "<!-- per signal it plots both a few possible templates as well as the average over 180 templates -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potential features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ecg_output = ecg.ecg(X_train.loc[ones[0]].dropna().to_numpy(dtype=\"float32\"))\n",
    "# print(\"classes: \", ecg_output[0].keys())\n",
    "# print(\"shape of time series: \", ecg_output[0][0].shape)\n",
    "# print(\"shape of filtered series: \", ecg_output[0][1].shape)\n",
    "# print(\"number of peaks\", ecg_output[0][2].shape[0])\n",
    "# print(\"shape of templates: \", ecg_output[0][3].shape)\n",
    "# print(\"peaks: \", ecg_output[0][2]/300)\n",
    "# print(\"templates: \", ecg_output[0][4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Heartbeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_features(signal):\n",
    "    r_peaks = ecg.engzee_segmenter(signal, 300)['rpeaks']\n",
    "    if len(r_peaks) >= 2:\n",
    "        beats = ecg.extract_heartbeats(signal, r_peaks, 300)['templates']\n",
    "        # print(\"Beats (peaks - 1, 180 proposals of heartbeats): \", beats)\n",
    "        if len(beats) != 0:\n",
    "            mu = np.mean(beats, axis=0) \n",
    "            var = np.std(beats, axis=0)\n",
    "            md = np.median(beats, axis=0)\n",
    "            \n",
    "            fig = plt.figure()\n",
    "            plt.subplot(211)\n",
    "\n",
    "            ax1 = plt.subplot(211)\n",
    "            ax1.plot(beats[0,:])\n",
    "            ax1.plot(beats[1,:])\n",
    "            ax1.plot(beats[2,:])\n",
    "            ax1.plot(beats[3,:])\n",
    "            ax1.set_title(\"template samples\")\n",
    "\n",
    "            ax2 = plt.subplot(212)\n",
    "            ax2.set_title(\"average template\")\n",
    "            ax2.plot(range(mu.shape[0]), mu, label='Average HeartBeat')\n",
    "            # Fill the variance range\n",
    "            ax2.fill_between(range(mu.shape[0]), mu - var, mu + var, linewidth=0, alpha=0.1)\n",
    "            # Plot a median\n",
    "            ax2.plot(range(md.shape[0]), md,  label='Median HeartBeat', color='#CC4F1B')\n",
    "    # ecg_results = ecg.ecg(signal, sampling_rate=300, show=True)\n",
    "    # return ecg_results\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract peaks and save as concatenates matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_peaks = 159\n",
    "peaks = np.zeros((max_num_peaks, threes.shape[0]))\n",
    "for i in range(threes.shape[0]):\n",
    "    data = X_train.loc[threes[i]].dropna().to_numpy(dtype=\"float32\")\n",
    "    ECG = ecg.ecg(data, sampling_rate=300, show=False)\n",
    "    ecg_dimension = ECG[2].shape[0]\n",
    "    buffer = np.zeros((max_num_peaks - ecg_dimension))\n",
    "    peaks[:,i] = np.concatenate((ECG[2], buffer), axis=0)\n",
    "np.savetxt(\"peaks/3/ecg_peaks.csv\", peaks, delimiter=\",\", fmt='%0.0f')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "peaks = pd.read_csv(\"peaks/4/ecg_peaks.csv\", header=None).to_numpy(dtype=\"float32\")\n",
    "zeros = np.where(peaks == 0)\n",
    "mean = np.mean(peaks, axis=0)\n",
    "std = np.std(peaks, axis=0)\n",
    "peaks_normalized = (peaks - mean) / std\n",
    "np.savetxt(\"peaks/4/ecg_peaks_normalized.csv\", peaks_normalized, delimiter=\",\", fmt='%0.7f')\n",
    "peaks_normalized[zeros] = 0\n",
    "np.savetxt(\"peaks/4/ecg_peaks_normalized_zeros.csv\", peaks_normalized, delimiter=\",\", fmt='%0.7f')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paper NN Structure:\n",
    "Automatic ECG Classification Using Continuous Wavelet\n",
    "Transform and Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Paper NN structure](Paper_pipeline.png \"Paper NN Structure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- insert image -->\n",
    "![Paper NN structure](Paper_NN_structure.png \"Paper NN Structure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.5921438342778972,\n",
       " 1: 0.08657416454954074,\n",
       " 2: 0.2880594098104358,\n",
       " 3: 0.03322259136212625}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize weights according to class distribution:\n",
    "data_size = 5117\n",
    "class_1_size = 3030\n",
    "class_2_size = 443\n",
    "class_3_size = 1474\n",
    "class_4_size = 170\n",
    "class_weights = {0: class_1_size/data_size, 1: class_2_size/data_size, 2: class_3_size/data_size, 3: class_4_size/data_size}\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "      keras.metrics.TruePositives(name='tp'),\n",
    "      keras.metrics.FalsePositives(name='fp'),\n",
    "      keras.metrics.TrueNegatives(name='tn'),\n",
    "      keras.metrics.FalseNegatives(name='fn'), \n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "]\n",
    "\n",
    "def create_feature_extractor_model(metrics=METRICS, output_bias=None):\n",
    "    '''Create feature extractor'''\n",
    "    if output_bias is not None:\n",
    "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "    #determine input shape\n",
    "    image_input = tf.keras.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
    "    encoder = tf.keras.Sequential()\n",
    "    encoder.add(keras.layers.Conv2D(filters=16,kernel_size=7,strides=1,input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)))\n",
    "    encoder.add(keras.layers.BatchNormalization())\n",
    "    encoder.add(keras.layers.ReLU())\n",
    "    encoder.add(keras.layers.MaxPool2D(5,5))\n",
    "    \n",
    "    encoder.add(keras.layers.Conv2D(32,3,1))\n",
    "    encoder.add(keras.layers.BatchNormalization())\n",
    "    encoder.add(keras.layers.ReLU())\n",
    "    encoder.add(keras.layers.MaxPool2D(3,3))\n",
    "\n",
    "    encoder.add(keras.layers.Conv2D(64,3,1))\n",
    "    encoder.add(keras.layers.BatchNormalization())\n",
    "    encoder.add(keras.layers.ReLU())\n",
    "    encoder.add(keras.layers.MaxPool2D(3))\n",
    "\n",
    "    # fully connected layer for scalar output\n",
    "    encoder.add(keras.layers.Flatten())\n",
    "    encoder.add(keras.layers.Dense(64, activation='relu'))\n",
    "    image_encoder = keras.Model(image_input, encoder(image_input))\n",
    "    # peaks encoder\n",
    "    peak_encoder = keras.Input(shape=(159))\n",
    "\n",
    "    complete_encoder = keras.layers.concatenate([image_encoder.output, peak_encoder])\n",
    "    classifier = keras.layers.Dense(32, activation='relu')(complete_encoder)\n",
    "    # Set initial bias towards class 1 and 3\n",
    "    classifier = keras.layers.Dense(4, activation='softmax', bias_initializer=output_bias)(classifier)\n",
    "    model = keras.Model([image_encoder.input, peak_encoder], classifier)\n",
    "\n",
    "    model.compile(\n",
    "      optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "      loss=keras.losses.BinaryCrossentropy(),\n",
    "      metrics=metrics)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Account for imbalanced dataset:\n",
    " Idea for imbalanced data: sample all classes to be of size 170 and proceed using ensemble method of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 4)\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 217, 334, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " sequential (Sequential)        (None, 64)           99744       ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 159)]        0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 223)          0           ['sequential[0][0]',             \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 32)           7168        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 4)            132         ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 107,044\n",
      "Trainable params: 106,820\n",
      "Non-trainable params: 224\n",
      "__________________________________________________________________________________________________\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "model = create_feature_extractor_model()\n",
    "print(model.output_shape)\n",
    "model.summary()\n",
    "# plot model\n",
    "keras.utils.plot_model(model, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = load_image(\"1/1.jpg\")\n",
    "# peaks = pd.read_csv(\"peaks/1/ecg_peaks_normalized_zeros.csv\", header=None).to_numpy(dtype=\"float32\")\n",
    "model.predict([image, peaks])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix seed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\fierz\\Documents\\LF\\studium\\Master\\3\\AML\\task2\\double_pipeline.ipynb Cell 34\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fierz/Documents/LF/studium/Master/3/AML/task2/double_pipeline.ipynb#X45sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrandom\u001b[39;00m \u001b[39mimport\u001b[39;00m seed\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fierz/Documents/LF/studium/Master/3/AML/task2/double_pipeline.ipynb#X45sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m seed(\u001b[39m1\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/fierz/Documents/LF/studium/Master/3/AML/task2/double_pipeline.ipynb#X45sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrand(\u001b[39m1\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "print(np.random.rand(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Larger Batch size to ensure that some examples of that class are in the batch\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 2048"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
