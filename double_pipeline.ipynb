{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task: Classification into 4 classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea is to use CWT images of the data and perform classification using CNNs\n",
    "\n",
    "### Use preprocessing.ipynb to preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from biosppy.signals import ecg\n",
    "from scipy import signal\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import os\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in class representatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3030\n",
      "443\n",
      "1474\n",
      "170\n"
     ]
    }
   ],
   "source": [
    "ones = pd.read_csv(\"class_reps/ones.csv\", header=None)\n",
    "twos = pd.read_csv(\"class_reps/twos.csv\", header=None)\n",
    "threes = pd.read_csv(\"class_reps/threes.csv\", header=None)\n",
    "fours = pd.read_csv(\"class_reps/fours.csv\", header=None)\n",
    "print(len(ones))\n",
    "print(len(twos))\n",
    "print(len(threes))\n",
    "print(len(fours))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paper NN Structure:\n",
    "Automatic ECG Classification Using Continuous Wavelet\n",
    "Transform and Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Paper NN structure](Paper_pipeline.png \"Paper NN Structure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- insert image -->\n",
    "![Paper NN structure](Paper_NN_structure.png \"Paper NN Structure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: load data\n",
    "def load_image(Class, index):\n",
    "    '''Helper Function to load the image'''\n",
    "    path = \"cwt_images/\" + str(Class) + \"/cwtmatr_class\" + str(Class) + \"_index\" + str(index) + \".png\"\n",
    "    img = tf.image.decode_png(tf.io.read_file(path), channels=3)\n",
    "    img = tf.cast(img, tf.float32)\n",
    "    img = img / 127.5 - 1\n",
    "    img = tf.image.resize(img, (IMG_HEIGHT, IMG_WIDTH))\n",
    "    # Don't Think data augmentation makes too much sense\n",
    "    # if training:\n",
    "    #     img = tf.image.random_flip_left_right(img)\n",
    "    #     img = tf.image.random_flip_up_down(img)\n",
    "    return img\n",
    "\n",
    "def load_input(Class, index):\n",
    "    '''Helper Function to load the image - peak pair'''\n",
    "    img = load_image(Class, index)\n",
    "    pk = pd.read_csv(\"peaks/\" + str(Class) + \"/ecg_peaks_normalized_zeros.csv\", header=None).to_numpy(dtype=\"float32\")[:,index]\n",
    "    image = tf.stack([img], axis=0)\n",
    "    peak = tf.stack([pk], axis=0)\n",
    "    Input = [image, peak]\n",
    "    return Input\n",
    "\n",
    "def load_data():\n",
    "    '''Load the dataset'''\n",
    "    \n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input = load_input(1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "      keras.metrics.TruePositives(name='tp'),\n",
    "      keras.metrics.FalsePositives(name='fp'),\n",
    "      keras.metrics.TrueNegatives(name='tn'),\n",
    "      keras.metrics.FalseNegatives(name='fn'), \n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "]\n",
    "IMG_HEIGHT = 217\n",
    "IMG_WIDTH = 334\n",
    "\n",
    "\n",
    "def create_model(metrics=METRICS, output_bias=None):\n",
    "    '''Create feature extractor'''\n",
    "    if output_bias is not None:\n",
    "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "\n",
    "    image_input = tf.keras.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
    "    encoder = tf.keras.Sequential()\n",
    "    encoder.add(keras.layers.Conv2D(filters=16,kernel_size=7,strides=1,input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)))\n",
    "    encoder.add(keras.layers.BatchNormalization())\n",
    "    encoder.add(keras.layers.ReLU())\n",
    "    encoder.add(keras.layers.MaxPool2D(5,5))\n",
    "    \n",
    "    encoder.add(keras.layers.Conv2D(32,3,1))\n",
    "    encoder.add(keras.layers.BatchNormalization())\n",
    "    encoder.add(keras.layers.ReLU())\n",
    "    encoder.add(keras.layers.MaxPool2D(3,3))\n",
    "\n",
    "    encoder.add(keras.layers.Conv2D(64,3,1))\n",
    "    encoder.add(keras.layers.BatchNormalization())\n",
    "    encoder.add(keras.layers.ReLU())\n",
    "    encoder.add(keras.layers.MaxPool2D(3))\n",
    "\n",
    "    # fully connected layer for scalar output\n",
    "    encoder.add(keras.layers.Flatten())\n",
    "    encoder.add(keras.layers.Dense(64, activation='relu'))\n",
    "    image_encoder = keras.Model(image_input, encoder(image_input))\n",
    "\n",
    "    \n",
    "    # peaks input\n",
    "    peak_input = keras.Input(shape=(159))\n",
    "\n",
    "    # conactenate the two submodels\n",
    "    complete_encoder = keras.layers.concatenate([image_encoder.output, peak_input])\n",
    "\n",
    "\n",
    "    classifier = keras.layers.Dense(32, activation='relu')(complete_encoder)\n",
    "    # Set initial bias towards class 1 and 3 to speed up convergence\n",
    "    classifier = keras.layers.Dense(4, activation='softmax', bias_initializer=output_bias)(classifier)\n",
    "    model = keras.Model([image_encoder.input, peak_input], classifier)\n",
    "\n",
    "    model.compile(\n",
    "      optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "      loss=keras.losses.BinaryCrossentropy(),\n",
    "      metrics=metrics)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix seed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.417022]\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "print(np.random.rand(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Bias to speed up convergence\n",
    "Note on the plotting of the model: If we want a more detailed overview we should define each layer separate and not in a sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = 5117\n",
    "class_1_size = ones.shape[0] #3030\n",
    "class_2_size = twos.shape[0]    #443\n",
    "class_3_size = threes.shape[0]  #1474\n",
    "class_4_size = fours.shape[0]   #170\n",
    "initial_bias = [class_1_size/data_size, class_2_size/data_size, class_3_size/data_size, class_4_size/data_size]\n",
    "\n",
    "model = create_model(output_bias=initial_bias)\n",
    "# model.summary()\n",
    "# keras.utils.plot_model(model, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction Test for the initialized bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image1 = load_image(1, 0)\n",
    "image2 = load_image(1, 1)\n",
    "\n",
    "peak1 = pd.read_csv(\"peaks/1/ecg_peaks_normalized_zeros.csv\", header=None).to_numpy(dtype=\"float32\")[:,0]\n",
    "peak2 = pd.read_csv(\"peaks/1/ecg_peaks_normalized_zeros.csv\", header=None).to_numpy(dtype=\"float32\")[:,1]\n",
    "\n",
    "# build images using tf.stack\n",
    "images = tf.stack([image1, image2], axis=0)\n",
    "peaks = tf.stack([peak1, peak2], axis=0)\n",
    "\n",
    "Input = [images,peaks]\n",
    "\n",
    "model1 = create_model()\n",
    "model2 = create_model(output_bias=initial_bias)\n",
    "print(\"Note the initial bias increases ikelihoods for class 1 and class 3\")\n",
    "print(model1.predict(Input))\n",
    "print(model2.predict(Input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the initial weights for later comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_weights = os.path.join(tempfile.mkdtemp(), 'initial_weights')\n",
    "model.save_weights(initial_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Might want Larger Batch size to ensure that some examples of that class are in the batch\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 1024 #2048\n",
    "baseline_history = model.fit(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "082e9a3bcad0a290d0001e938aa60b99250c6c2ef33a923c00b70f9826caf4b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
