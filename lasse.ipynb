{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task: Classification into 4 classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea is to use CWT images of the data and perform classification using CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes:\n",
    "- Can't drop nans collectively because of different signal lengths\n",
    "- For the imbalanced datasets we can introduce some bias towards the under-represented values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from biosppy.signals import ecg\n",
    "from scipy import signal\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import â€“ takes a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# import X_train.csv dataset into a pandas dataframe\n",
    "X_train = pd.read_csv(\"X_train.csv\", index_col=\"id\")\n",
    "y_train = pd.read_csv(\"y_train.csv\", index_col=\"id\")\n",
    "X_test = pd.read_csv(\"X_test.csv\", index_col=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.164318\n",
      "16322\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(X_train.loc[0].dropna().to_numpy(dtype=\"float32\")))\n",
    "print(X_train.loc[0].dropna().to_numpy(dtype=\"float32\").shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get class representatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3030\n",
      "443\n",
      "1474\n",
      "170\n"
     ]
    }
   ],
   "source": [
    "y_train_np = y_train.to_numpy()\n",
    "ones = np.where(y_train_np == 0)[0]\n",
    "twos = np.where(y_train_np == 1)[0]\n",
    "threes = np.where(y_train_np == 2)[0]\n",
    "fours = np.where(y_train_np == 3)[0]\n",
    "print(len(ones))\n",
    "print(len(twos))\n",
    "print(len(threes))\n",
    "print(len(fours))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Signal Plot attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_trimmed_0 = X_train.loc[0,].dropna().to_numpy(dtype=\"float32\")\n",
    "# # print(\"Training data shape: \", X_train.loc[0].dropna().to_numpy(dtype=\"float32\").shape)\n",
    "# x = np.linspace(0,X_train_trimmed_0.shape[0],X_train_trimmed_0.shape[0])\n",
    "# plt.plot(x, X_train_trimmed_0)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Wavelet transform:\n",
    "### Create CWT images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# widths = np.arange(1,31)\n",
    "# print(\"cwtmatrix shape (30 from width x # signal length\")\n",
    "# Class = 3\n",
    "# for i in range(len(threes)):\n",
    "#         print(i)\n",
    "#         # print(len(X_train.loc[threes[i]].dropna().to_numpy(dtype=\"float32\")))\n",
    "#         cwtmatr = signal.cwt(X_train.loc[threes[i]].dropna().to_numpy(dtype=\"float32\"), signal.ricker, widths)\n",
    "#         # print(cwtmatr.shape)\n",
    "#         plt.figure()\n",
    "#         plt.imshow(cwtmatr, extent=[-1, 1, 31, 1], cmap='PRGn', aspect='auto',\n",
    "#                 vmax=abs(cwtmatr).max(), vmin=-abs(cwtmatr).max())\n",
    "#         plt.axis('off')\n",
    "#         # plt.show()\n",
    "#         plt.savefig(\"3/cwtmatr_class{}_index{}.png\".format(Class, i), bbox_inches='tight', pad_inches=0)\n",
    "#         plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH = 334\n",
    "IMG_HEIGHT = 217\n",
    "def load_image(img, training):\n",
    "    '''Helper Function to load the image'''\n",
    "    img = tf.image.decode_png(tf.io.read_file(img), channels=3)\n",
    "    img = tf.cast(img, tf.float32)\n",
    "    img = img / 127.5 - 1\n",
    "    img = tf.image.resize(img, (IMG_HEIGHT, IMG_WIDTH))\n",
    "    if training:\n",
    "        img = tf.image.random_flip_left_right(img)\n",
    "        img = tf.image.random_flip_up_down(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 16:42:48.104928: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "image_1 = load_image(\"3/cwtmatr_class3_index0.png\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Account for imbalanced dataset:\n",
    " Idea for imbalanced data: sample all classes to be of size 170 and proceed using ensemble method of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paper NN Structure:\n",
    "Automatic ECG Classification Using Continuous Wavelet\n",
    "Transform and Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Paper NN structure](Paper_pipeline.png \"Paper NN Structure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- insert image -->\n",
    "![Paper NN structure](Paper_NN_structure.png \"Paper NN Structure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_extractor_model():\n",
    "    '''Create feature extractor'''\n",
    "    #determine input shape\n",
    "    model = keras.Sequential([\n",
    "        tf.keras.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
    "        tf.keras.layers.Conv2D(filters=16,kernel_size=7,strides=1,input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.ReLU(),\n",
    "        tf.keras.layers.MaxPool2D(5,5),\n",
    "        \n",
    "        tf.keras.layers.Conv2D(32,3,1),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.ReLU(),\n",
    "        tf.keras.layers.MaxPool2D(3,3),\n",
    "\n",
    "        tf.keras.layers.Conv2D(64,3,1),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.ReLU(),\n",
    "        tf.keras.layers.MaxPool2D(3),\n",
    "\n",
    "        # fully connected layer for scalar output\n",
    "        tf.keras.layers.Flatten(),\n",
    "        # Concatenation layer\n",
    "    ])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 1152)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 211, 328, 16)      2368      \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 211, 328, 16)     64        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " re_lu (ReLU)                (None, 211, 328, 16)      0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 42, 65, 16)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 40, 63, 32)        4640      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 40, 63, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_1 (ReLU)              (None, 40, 63, 32)        0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 13, 21, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 11, 19, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 11, 19, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_2 (ReLU)              (None, 11, 19, 64)        0         \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 3, 6, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1152)              0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25,952\n",
      "Trainable params: 25,728\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-27 16:53:02.821041: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = create_feature_extractor_model()\n",
    "print(model.output_shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToDo: \n",
    "- Extract features from heartbeat templates and concatenate with scalar output from model\n",
    "- consider imbalanced dataset => add bias towards under-represented classes\n",
    "- train model\n",
    "- predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction: \n",
    "<!-- per signal it plots both a few possible templates as well as the average over 180 templates -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_features(signal):\n",
    "#     r_peaks = ecg.engzee_segmenter(signal, 300)['rpeaks']\n",
    "#     if len(r_peaks) >= 2:\n",
    "#         beats = ecg.extract_heartbeats(signal, r_peaks, 300)['templates']\n",
    "#         # print(\"Beats (peaks - 1, 180 proposals of heartbeats): \", beats)\n",
    "#         if len(beats) != 0:\n",
    "#             # for i in range(5):\n",
    "#             #     plt.plot(beats[i])\n",
    "#             #     plt.show()\n",
    "#             mu = np.mean(beats, axis=0) \n",
    "#             var = np.std(beats, axis=0)\n",
    "#             md = np.median(beats, axis=0)\n",
    "            \n",
    "#             fig = plt.figure()\n",
    "#             plt.subplot(211)\n",
    "\n",
    "#             ax1 = plt.subplot(211)\n",
    "#             ax1.plot(beats[0,:])\n",
    "#             ax1.plot(beats[1,:])\n",
    "#             ax1.plot(beats[2,:])\n",
    "#             ax1.plot(beats[3,:])\n",
    "#             ax1.set_title(\"template samples\")\n",
    "\n",
    "#             ax2 = plt.subplot(212)\n",
    "#             ax2.set_title(\"average template\")\n",
    "#             ax2.plot(range(mu.shape[0]), mu, label='Average HeartBeat')\n",
    "#             # Fill the variance range\n",
    "#             ax2.fill_between(range(mu.shape[0]), mu - var, mu + var, linewidth=0, alpha=0.1)\n",
    "#             # Plot a median\n",
    "#             ax2.plot(range(md.shape[0]), md,  label='Median HeartBeat', color='#CC4F1B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_features(signal):\n",
    "    r_peaks = ecg.engzee_segmenter(signal, 300)['rpeaks']\n",
    "    if len(r_peaks) >= 2:\n",
    "        beats = ecg.extract_heartbeats(signal, r_peaks, 300)['templates']\n",
    "        # print(\"Beats (peaks - 1, 180 proposals of heartbeats): \", beats)\n",
    "        if len(beats) != 0:\n",
    "            mu = np.mean(beats, axis=0) \n",
    "            var = np.std(beats, axis=0)\n",
    "            md = np.median(beats, axis=0)\n",
    "            \n",
    "            fig = plt.figure()\n",
    "            plt.subplot(211)\n",
    "\n",
    "            ax1 = plt.subplot(211)\n",
    "            ax1.plot(beats[0,:])\n",
    "            ax1.plot(beats[1,:])\n",
    "            ax1.plot(beats[2,:])\n",
    "            ax1.plot(beats[3,:])\n",
    "            ax1.set_title(\"template samples\")\n",
    "\n",
    "            ax2 = plt.subplot(212)\n",
    "            ax2.set_title(\"average template\")\n",
    "            ax2.plot(range(mu.shape[0]), mu, label='Average HeartBeat')\n",
    "            # Fill the variance range\n",
    "            ax2.fill_between(range(mu.shape[0]), mu - var, mu + var, linewidth=0, alpha=0.1)\n",
    "            # Plot a median\n",
    "            ax2.plot(range(md.shape[0]), md,  label='Median HeartBeat', color='#CC4F1B')\n",
    "    # ecg_results = ecg.ecg(signal, sampling_rate=300, show=True)\n",
    "    # return ecg_results\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(5):\n",
    "    # plot_features(X_train.loc[ones[i]].dropna().to_numpy(dtype=\"float32\"))\n",
    "    # plot_features(X_train.loc[twos[i]].dropna().to_numpy(dtype=\"float32\"))\n",
    "    # plot_features(X_train.loc[threes[i]].dropna().to_numpy(dtype=\"float32\"))\n",
    "    # plot_features(X_train.loc[fours[i]].dropna().to_numpy(dtype=\"float32\"))\n",
    "max = 0\n",
    "for i in range(fours.shape[0]):\n",
    "    data = X_train.loc[fours[i]].dropna().to_numpy(dtype=\"float32\")\n",
    "    ECG = ecg.ecg(data, sampling_rate=300, show=False)\n",
    "    if ECG[2].shape[0] > max:\n",
    "        max = ECG[2].shape[0]\n",
    "    # np.savetxt(\"peaks/4/ecg_peaks_class1_index\" + str(i) + \".csv\", ECG[2], delimiter=\",\", fmt='%0.0f')\n",
    "# data = X_train.loc[0].dropna().to_numpy(dtype=\"float32\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potential features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes:  ['ts', 'filtered', 'rpeaks', 'templates_ts', 'templates', 'heart_rate_ts', 'heart_rate']\n",
      "shape of time series:  (16322,)\n",
      "shape of filtered series:  (16322,)\n",
      "number of peaks 66\n",
      "shape of templates:  (180,)\n",
      "peaks:  [ 0.33666667  0.66333333  1.49666667  2.31666667  3.16333333  4.01333333\n",
      "  4.86333333  5.7         6.55333333  7.4         8.23666667  9.07666667\n",
      "  9.92       10.76333333 11.58666667 12.42       13.27       14.10333333\n",
      " 14.92666667 15.75666667 16.57       17.37666667 18.17666667 18.97333333\n",
      " 19.76333333 20.54333333 21.33       22.11666667 22.90666667 23.68333333\n",
      " 24.44       25.2        25.98       26.78333333 27.59       28.39666667\n",
      " 29.22333333 30.04666667 30.88333333 31.71666667 32.54       33.37666667\n",
      " 34.20333333 35.03       35.85       36.68666667 37.52333333 38.35666667\n",
      " 39.18666667 40.01666667 40.85       41.68333333 42.52333333 43.36666667\n",
      " 44.19       45.01333333 45.84666667 46.68666667 47.53       48.36333333\n",
      " 49.2        50.04       50.87666667 51.70666667 52.54       53.37      ]\n",
      "templates:  [[  4.96145956 -12.17807597 -26.45706419 ... -70.44953929 -64.0175891\n",
      "  -56.32118143]\n",
      " [-58.47820539 -63.02492934 -65.48029486 ... -30.03214977 -29.8327222\n",
      "  -31.06927763]\n",
      " [-12.85068568  -5.8165549    1.26662213 ... -15.68802045 -12.46552804\n",
      "   -9.61460685]\n",
      " ...\n",
      " [-10.64237365 -11.05802108 -11.54461395 ... -14.34140685 -12.19505748\n",
      "  -10.37076943]\n",
      " [ -4.08851427  -1.88813893   0.6468594  ... -23.32734556 -19.66626461\n",
      "  -15.80872864]\n",
      " [ -3.77074272  -4.705999    -5.89398021 ... -34.499976   -39.10669108\n",
      "  -42.56327718]]\n"
     ]
    }
   ],
   "source": [
    "print(\"classes: \", ecg_output[0].keys())\n",
    "print(\"shape of time series: \", ecg_output[0][0].shape)\n",
    "print(\"shape of filtered series: \", ecg_output[0][1].shape)\n",
    "print(\"number of peaks\", ecg_output[0][2].shape[0])\n",
    "print(\"shape of templates: \", ecg_output[0][3].shape)\n",
    "print(\"peaks: \", ecg_output[0][2]/300)\n",
    "print(\"templates: \", ecg_output[0][4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "082e9a3bcad0a290d0001e938aa60b99250c6c2ef33a923c00b70f9826caf4b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
